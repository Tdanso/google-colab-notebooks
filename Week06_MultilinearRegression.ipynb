{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week06_MultilinearRegression",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tdanso/google-colab-notebooks/blob/main/Week06_MultilinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXhaotNQxr2w"
      },
      "source": [
        "# Week 6\n",
        "# Multilinear Regression\n",
        "\n",
        "Last time we looked at a simple linear regression model $sales = \\beta_0 + \\beta_1\\cdot\\textit{TV advertising budget}$. More generally, a linear model makes a prediction by computing a weighted sum of their input features (plus a constant).\n",
        "\n",
        "**Reading: Chapter 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81lHyTpwx0zu"
      },
      "source": [
        "## Multilinear Regression: Model Assumptions\n",
        "**Model**:\n",
        "\n",
        "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$\n",
        "1. $\\hat{y}$ is the predicted value.\n",
        "2. $n$ is the number of features.\n",
        "3. $x_i$ is the i-th feature value.\n",
        "4. $\\theta_j$ is the j-th model parameter (associated with $x_j$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azLxHqqUx02U"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5RGmIdx04v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "b57e2097-ffe5-4849-8411-2bf84d89d995"
      },
      "source": [
        "# Toy example\n",
        "columns = ['Homework', 'Midterm', 'Final']\n",
        "data = pd.DataFrame({\n",
        "    \"Homework\": [95, 70, 80, 100, 70],\n",
        "    \"Midterm\": [90, 60, 80, 80, 85],\n",
        "    \"Final\": [93, 66, 85, 60, 90]\n",
        "}, index=[\"Alice\", \"Bob\", \"Clare\", \"David\", \"Eve\"])\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Homework</th>\n",
              "      <th>Midterm</th>\n",
              "      <th>Final</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Alice</th>\n",
              "      <td>95</td>\n",
              "      <td>90</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bob</th>\n",
              "      <td>70</td>\n",
              "      <td>60</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Clare</th>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>David</th>\n",
              "      <td>100</td>\n",
              "      <td>80</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Eve</th>\n",
              "      <td>70</td>\n",
              "      <td>85</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Homework  Midterm  Final\n",
              "Alice        95       90     93\n",
              "Bob          70       60     66\n",
              "Clare        80       80     85\n",
              "David       100       80     60\n",
              "Eve          70       85     90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKguV_HWx09V"
      },
      "source": [
        "In this case:\n",
        "- $x_1$ is the homework feature\n",
        "- $x_2$ is the midterm feature\n",
        "- $y$ is the final feature\n",
        "- model is: $final = \\theta_0 + \\theta_1 * homework + \\theta_2 * midterm$\n",
        "- We need to come up with values for $\\theta_0, \\theta_1, \\theta_2$ to complete the model.\n",
        "\n",
        "**Objective**: Suppose that another student Fred has Homework score 85 and Midterm score 80. What is prediction of his final exam score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFgiEHAx0_q"
      },
      "source": [
        "## Multilinear Regression: Vectorized form\n",
        "\n",
        "The multilinear model can also be written as:\n",
        "\n",
        "**$\\hat{y} = \\theta\\cdot\\textbf{x}$**.\n",
        "1. $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$ is the paramter vector.\n",
        "2. $\\textbf{x} = (1, x_1, ..., x_n)$ is the feature vector.\n",
        "3. The symbol $\\cdot$ represents the inner-product of two vectors. For example, $(1, 2, 3)\\cdot (4, 5, 6) = 1\\times 4 + 2\\times 5 + 3\\times 6 = 32$.\n",
        "\n",
        "**Why is the expression $\\theta\\cdot\\textbf{x}$ equivalent to $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Acxyvfx1CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea21f3e-197c-45c2-b8ca-e952bf31f8a9"
      },
      "source": [
        "# Let's apply the linear regression tool in sci-kit learn on the toy example\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "model_lr.fit(data[[\"Homework\", \"Midterm\"]], data[[\"Final\"]]) \n",
        "# The input data requires two layers of brackets because the input features should be expressed as a list."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoD9GMG1x1Em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c928d48-13f8-43ba-bab0-7fc51d32ec25"
      },
      "source": [
        "# Retrieve the estimated parameter values.\n",
        "print(\"Theta 0:\", model_lr.intercept_)\n",
        "print(\"Theta 1 and Theta 2:\", model_lr.coef_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta 0: [35.]\n",
            "Theta 1 and Theta 2: [[-0.71627907  1.30697674]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcqnAPn8Hd6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296fa58b-4c5a-4ac6-bfe5-0856c807f8fb"
      },
      "source": [
        "# Apply the model to provide prediction for Fred\n",
        "model_lr.predict([[85, 80],\n",
        "                  [60, 60]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[78.6744186 ],\n",
              "       [70.44186047]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYP9E1UoIBNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba20ac9d-c810-4237-9ac9-9d3e7b26b0b1"
      },
      "source": [
        "# Remember prediction = theta0 + theta1 * homework + theta2 * midterm\n",
        "theta0 = 35\n",
        "theta1 = -0.716\n",
        "theta2 = 1.307\n",
        "\n",
        "prediction = theta0 + theta1 * 60 + theta2 * 60\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70.46000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFWHQvs4IkxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8fd78f-e109-4d2e-b03d-b8ae63fb628b"
      },
      "source": [
        "# Let's use the vector form to get the prediction.\n",
        "# prediction = inner-product of the parameter vector and the feature vector.\n",
        "parameter_vector = np.array([35, -0.716, 1.307])\n",
        "feature_vector = np.array([1, 60, 60])\n",
        "\n",
        "prediction = parameter_vector.dot(feature_vector)\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwJG44-7x1HE"
      },
      "source": [
        "## Multilinear Regression: Cost Function\n",
        "In order to calculate the best value for each parameter, we need a **cost function** that evaluates the errors made by a give set of parameter values. Here we use the **mean squared error (MSE)** function as the cost function:\n",
        "\n",
        "$J(\\textbf{X}, \\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big(\\theta\\cdot\\textbf{x}^{(i)} - y^{(i)}\\big)^2$\n",
        "\n",
        "Here $(\\textbf{x}^{(i)}, y^{(i)})$ represents the i-th training example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh-Zy_vEx1JZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecba555-f48b-47f6-cdf3-3d378cca8f65"
      },
      "source": [
        "# Calculate the MSE cost of the toy example for the parameter values given by sci-kit learn.\n",
        "\n",
        "theta = np.array([35, -0.716, 1.307])\n",
        "\n",
        "list_errors = []\n",
        "\n",
        "for i in data.index:\n",
        "    # print(i)\n",
        "    x = np.array([1, data.loc[i, \"Homework\"], data.loc[i, \"Midterm\"]])\n",
        "    theta_dot_x = theta.dot(x)\n",
        "    y = data.loc[i, \"Final\"]\n",
        "    squared_error = (theta_dot_x - y) ** 2\n",
        "    list_errors.append(squared_error)\n",
        "\n",
        "print(list_errors)\n",
        "print(\"MSE:\", np.mean(list_errors))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[70.39210000000001, 7.290000000000015, 7.398399999999993, 63.361600000000124, 35.70062499999993]\n",
            "MSE: 36.82854500000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG2rW2SNx1Lw"
      },
      "source": [
        "## Multilinear Regression: Training Algorithm 1\n",
        "The value of $\\theta$ that minimizes the cost function is given by the following **normal equation**:\n",
        "\n",
        "$\\hat{\\theta} = \\big(\\textbf{X}^T\\cdot\\textbf{X}\\big)^{-1}\\cdot\\textbf{X}^T\\cdot\\textbf{y}$.\n",
        "\n",
        "1. $\\textbf{X}$ is an $m\\times (n+1)$ matrix whose i-th row is $\\textbf{x}^{(i)}$.\n",
        "$$\\textbf{X} = \\begin{pmatrix}\n",
        "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
        "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
        "\\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n \\\\\n",
        "\\end{pmatrix}$$\n",
        "2. $$\\textbf{y} = \\begin{pmatrix}y^{(1)} \\\\ \\vdots \\\\ y^{(m)}\\end{pmatrix}$$.\n",
        "3. The cost function $J(\\theta)$ also has a matrix expression\n",
        "$$J(\\theta) = \\frac{1}{m}(\\textbf{X}\\cdot\\theta - \\textbf{y})^T\\cdot (\\textbf{X}\\cdot\\theta - \\textbf{y})$$\n",
        "\n",
        "- $m$ represents the number of records in the training set.\n",
        "- $n$ represents the number of input variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0r86Sigx1OO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7f9a11-fb02-4ffb-c93e-602add4801cd"
      },
      "source": [
        "# Construct matrix X using np.hstack(), np.ones()\n",
        "\n",
        "# 1. Construct a column of ones\n",
        "\n",
        "X = np.hstack([np.ones([5, 1]), data[[\"Homework\", \"Midterm\"]].values])\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  1.  95.  90.]\n",
            " [  1.  70.  60.]\n",
            " [  1.  80.  80.]\n",
            " [  1. 100.  80.]\n",
            " [  1.  70.  85.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIZZCIicx1Q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544c8afc-21c6-4128-e7ca-496f95b4fad3"
      },
      "source": [
        "# Construct vector y\n",
        "y = data[[\"Final\"]].values\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[93]\n",
            " [66]\n",
            " [85]\n",
            " [60]\n",
            " [90]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "197CDh19x1Th",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9d15ee-e175-4efe-93bb-a1782e6b1cbb"
      },
      "source": [
        "# Apply the normal equation to find theta\n",
        "\n",
        "theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "print(\"Theta:\", theta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta: [[35.        ]\n",
            " [-0.71627907]\n",
            " [ 1.30697674]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ezXjZlvx1WG"
      },
      "source": [
        "## Multilinear Regression: Training Algorithm 2\n",
        "The normal equation is not applicable when $\\textbf{X}^T\\cdot\\textbf{X}$ is not invertible. It happens if:\n",
        "- Several features are linearly dependent (for example, feature3 = feature1 + feature2)\n",
        "- The number of features is greater than the number of training data (for example, DNA data)\n",
        "\n",
        "When the matrix $\\textbf{X}$ is too large, the normal equation may take too long to finish since it requires a matrix multiplication.\n",
        "\n",
        "In these cases, we can use the **gradient descent** method to minimize the cost function instead.\n",
        "\n",
        "Gradient descent with one variable ideally looks like this:\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/0*fU8XFt-NCMZGAWND.\" width=\"600\">\n",
        "\n",
        "Gradient descent with two variables ideally looks like this:\n",
        "<img src=\"https://blog.paperspace.com/content/images/2019/09/F1-02.large.jpg\" width=\"600\">\n",
        "\n",
        "Gradient descent is an iterative algorithm for finding the **local minimum** of a differentiable function.\n",
        "- Choose an initial value of $\\hat{\\theta}$ and a **learning rate** $r$.\n",
        "- For each iteration $k$, do:\n",
        "    - Calculate the partial derivative of the cost function:\n",
        "    $$\n",
        "    \\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta} = \\frac{2}{m}\\cdot\\textbf{X}^T\\cdot(\\textbf{X}\\cdot\\theta - \\textbf{y}).\n",
        "    $$\n",
        "    - Update the parameter vector:\n",
        "    $$\\hat{\\theta} \\leftarrow \\hat{\\theta} - r\\cdot\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta}.$$\n",
        "\n",
        "- **Verify the formula of partial derivative asuuming there is one input feature.**\n",
        "\n",
        "- End iteration if certain stop criteria is reached, such as:\n",
        "    - Value of $\\hat{\\theta}$ becomes stable.\n",
        "    - Certain iteration amount is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clRpPwu-x1Yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb219c25-15cf-4cc4-a836-c1c8e78b6c9c"
      },
      "source": [
        "# Choose a random initial value for each parameter.\n",
        "\n",
        "theta = np.array([10, 1, 0.1]).reshape([3, 1]) # reshape is needed since theta should be a column vector.\n",
        "print(theta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10. ]\n",
            " [ 1. ]\n",
            " [ 0.1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yP5gFqHx1bM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50f803b-7d89-4401-9a1e-0a5ca155553b"
      },
      "source": [
        "# Perform gradient descent once.\n",
        "# Choose a learning rate r\n",
        "r = 0.00001\n",
        "\n",
        "# 1. Calculate the gradient\n",
        "gradient = 2 / 5 * (X.T).dot(X.dot(theta) - y)\n",
        "print(\"gradient:\", gradient)\n",
        "\n",
        "# 2. Update the parameters\n",
        "theta = theta - r * gradient\n",
        "print(\"theta:\", theta)\n",
        "\n",
        "# 3. (optional) Show the MSE cost with new parameter values\n",
        "mat = X.dot(theta) - y\n",
        "MSE = 1/5 * mat.T.dot(mat)\n",
        "print(\"MSE:\", MSE)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradient: [[  -1.43487687]\n",
            " [ 168.49692283]\n",
            " [-177.62272499]]\n",
            "theta: [[9.99983022]\n",
            " [0.66752705]\n",
            " [0.16048696]]\n",
            "MSE: [[272.86952431]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPqDCTznx1dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d076c8-5bd9-4f68-f2d9-52b5ef443945"
      },
      "source": [
        "# Perform gradient descent multiple times\n",
        "# Start from scratch\n",
        "\n",
        "# Initialization of theta\n",
        "theta = np.array([10, 1, 0]).reshape([3, 1])\n",
        "r = 0.00006\n",
        "\n",
        "history = []\n",
        "\n",
        "for i in range(1000): # 6 million will get to the global minimum.\n",
        "\n",
        "    # Calculate the gradient with current value of theta\n",
        "    gradient = 2 / 5 * X.T.dot(X.dot(theta) - y)\n",
        "\n",
        "    # Update theta with the gradient\n",
        "    theta = theta - r * gradient\n",
        "\n",
        "    # (optional) Calculate and display the MSE cost for the new theta\n",
        "    mat = X.dot(theta) - y\n",
        "    MSE = 1/5 * mat.T.dot(mat)\n",
        "    history.append(MSE[0, 0])\n",
        "\n",
        "print(\"Theta:\")\n",
        "print(theta)\n",
        "print(\"MSE:\", MSE)\n",
        "\n",
        "# Here we skipped the normalization of data, so the gradient descent takes way \n",
        "# too long to find the minimum."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theta:\n",
            "[[10.04741269]\n",
            " [-0.61164291]\n",
            " [ 1.50847835]]\n",
            "MSE: [[45.53715029]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfCU6C33reK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "8a3065d4-ee20-4bf5-fb0e-78bd74f4914a"
      },
      "source": [
        "# Plot the learning curve.\n",
        "\n",
        "plt.plot(range(1000), history, 'b.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9e4d482550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOFqVwlx1gQ"
      },
      "source": [
        "**Discussion**\n",
        "1. Change $r$ to 0.000001 and 1. Observe the MSE curve.\n",
        "\n",
        "Answer: A large learning rate will likely make the parameter go over the minimum. As a result, the learning curve will explode.\n",
        "\n",
        "2. Do the initial parameter values matter?\n",
        "\n",
        "The initial values determine the path of the gradient descent.\n",
        "\n",
        "3. How to determine when to stop the iteration?\n",
        "- It depends the time we can afford.\n",
        "- If the learning curve becomes flat to a long time, it indicates that the MSE cannot be lower much more.\n",
        "- If the MSE value stablizes, it may imply that it has reached the minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEnnJd4Qx1ii"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWfuoUHx1lB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfSS1dgx1np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}